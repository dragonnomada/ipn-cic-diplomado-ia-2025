{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437e3b5a",
   "metadata": {},
   "source": [
    "# Práctica 4 - Tokenización Unigram\n",
    "\n",
    "> **Intituto Politécnico Nacional**\n",
    ">\n",
    "> **Centro de Investigación en Computación**\n",
    ">\n",
    "> \n",
    "> \n",
    "> Departamento de Diplomados y Extensión Profesional\n",
    "> \n",
    "> Diplomado en Inteligencia Artificial\n",
    "> \n",
    "> * Módulo 11 - Parte I\n",
    "> \n",
    "> Profesor: Alan Badillo Salas (badillosalas@outlook.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3766bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample corpus file\n",
    "corpus = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
    "with open(\"resultados/corpus_hug.txt\", \"w\") as f:\n",
    "    for word in corpus:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: resultados/corpus_hug.txt\n",
      "  input_format: \n",
      "  model_prefix: resultados/unigram_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 13\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: resultados/corpus_hug.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 5 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=21\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=8\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 5 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=16\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 13 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 5\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 5\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 5 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8 obj=5.89287 num_tokens=11 num_tokens/piece=1.375\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8 obj=5.25194 num_tokens=11 num_tokens/piece=1.375\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: resultados/unigram_tokenizer.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: resultados/unigram_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Step 1: Train a Unigram tokenizer\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=\"resultados/corpus_hug.txt\",\n",
    "    model_prefix=\"resultados/unigram_tokenizer\",\n",
    "    vocab_size=13,  # Adjust based on desired vocabulary size\n",
    "    model_type=\"unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e7257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load the trained Unigram model\n",
    "sp = spm.SentencePieceProcessor(model_file=\"resultados/unigram_tokenizer.model\")\n",
    "\n",
    "sp.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95048c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: hug -> Tokens: ['▁hug']\n",
      "Word: pug -> Tokens: ['▁pu', 'g']\n",
      "Word: pun -> Tokens: ['▁pu', 'n']\n",
      "Word: hugs -> Tokens: ['▁hug', 's']\n",
      "Word: bun -> Tokens: ['▁', 'b', 'u', 'n']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenize text with the Unigram model\n",
    "words = [\"hug\", \"pug\", \"pun\", \"hugs\", \"bun\"]\n",
    "for word in words:\n",
    "    tokens = sp.encode(word, out_type=str)\n",
    "    print(f\"Word: {word} -> Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b7039f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [4] -> Decoded word: hug\n",
      "Tokens: [5, 10] -> Decoded word: pug\n",
      "Tokens: [5, 3] -> Decoded word: pun\n",
      "Tokens: [4, 12] -> Decoded word: hugs\n",
      "Tokens: [7, 8, 9, 3] -> Decoded word: bun\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Decode tokens back to text\n",
    "for word in words:\n",
    "    token_ids = sp.encode(word)\n",
    "    decoded_word = sp.decode(token_ids)\n",
    "    print(f\"Tokens: {token_ids} -> Decoded word: {decoded_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e427de7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(\"bug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35da8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bug'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([7, 8, 9, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
