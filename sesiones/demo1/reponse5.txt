```json
["The Transformer model uses only attention mechanisms, eliminating recurrence and convolutions.", "Transformer models outperform traditional models in quality and training efficiency.", "Transformer achieved 28.4 BLEU on WMT 2014 English-to-German translation, surpassing previous bests.", "On WMT 2014 English-to-French translation, Transformer set a new BLEU record with a score of 41.0.", "Training Transformers is more parallelizable and requires less time compared to other models."]
```