```
La atención es todo lo que necesitas

Resumen

Los modelos dominantes de <abbr title="transducción de secuencia">sequence transduction</abbr> se basan en redes neuronales complejas <abbr title="recurrentes">recurrent</abbr> o convolucionales que incluyen un codificador y un decodificador. Los modelos que mejor funcionan también conectan el codificador y el decodificador a través de un mecanismo de atención. Proponemos una nueva arquitectura de red simple, el Transformer, basada únicamente en mecanismos de atención, prescindiendo por completo de la recurrencia y las convoluciones. Los experimentos en dos tareas de traducción automática muestran que estos modelos son superiores en calidad mientras son más <abbr title="capacidad de realizar múltiples operaciones simultáneamente">paralelizables</abbr> y requieren significativamente menos tiempo para entrenar. Nuestro modelo alcanza 28.4 <abbr title="métrica para evaluar traducciones automáticas">BLEU</abbr> en la tarea de traducción de inglés a alemán de <abbr title="Workshop on Machine Translation">WMT</abbr> 2014, mejorando los resultados existentes, incluidos los <abbr title="conjuntos de modelos">ensembles</abbr>, en más de 2 <abbr title="métrica para evaluar traducciones automáticas">BLEU</abbr>. En la tarea de traducción de inglés a francés de <abbr title="Workshop on Machine Translation">WMT</abbr> 2014, nuestro modelo establece un nuevo <abbr title="estado del arte">state-of-the-art</abbr> de puntuación <abbr title="métrica para evaluar traducciones automáticas">BLEU</abbr> de 41.0 después de entrenar durante 3.5 días en ocho <abbr title="Unidad de Procesamiento Gráfico">GPUs</abbr>, una pequeña fracción de los costos de entrenamiento de los mejores modelos de la literatura.
```